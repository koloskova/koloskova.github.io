<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Anastasia Koloskova</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="shortcut icon" href="images/avatar.jpeg" type="image/x-icon">
    <meta name="description" content="Anastasia Koloskova, PhD student in Machine Learning at EPFL."/>
		<meta name="google-site-verification" content="Mgk0lh1zGVm-zr8CV7vrvmdZqAGgz4-yB7KyKCq7sfE" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar"><img src="images/avatar.jpeg" alt="" /></a>
					<h1><strong> Anastasia Koloskova </strong></h1>
						<br>

						<p><a style="color:#a2a2a2" class="icon solid fa-envelope"><span class="label">Email</span></a>&nbsp&nbsp anakolos[at]stanford[dot]edu</p>
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="one">
						<header class="major">
							<h2>About me</h2>
						</header>
						<p> <b>I will be joining the <a href="https://dm3l.uzh.ch/home">University of Zurich</a> as an assistant professor in August this year. I am looking for PhD students and postdocs starting from the fall. </b> If you are interested, see below for how to apply.
						Currently, I am a postdoc at Stanford University at <a href=https://stairlab.stanford.edu/>STAIR lab</a> with <a href=https://cs.stanford.edu/~sanmi/>Prof. Sanmi Koyejo</a>. I'm interested in machine learning, optimization, and their intersections with decentralized and collaborative learning, and privacy.</br></br>

							Previously, I completed my Ph.D. at <a href="https://www.epfl.ch/en/">EPFL</a>, Switzerland in the 
							laboratory of Machine Learning and Optimization (<a href="https://www.epfl.ch/labs/mlo/">MLO</a>), 
							supervised by <a href="https://people.epfl.ch/martin.jaggi?lang=en">Prof. Martin Jaggi</a>.
							My PhD was supported by a <a href="https://research.google/outreach/phd-fellowship/">Google PhD Fellowship</a> in Machine Learning. During my PhD I also spent some time at FAIR (Facebook AI Research), and Google Research. My PhD thesis received <a href="https://ellis.eu/news/the-ellis-phd-award-2024-recognizes-three-young-scientists-from-universities-in-lausanne-london-tubingen">ELLIS PhD Awards</a> and <a href="https://www.epfl.ch/education/phd/phd-awards/thesis-distinction/">EPFL Thesis Distinction Award</a>.
						</p>
						<!-- <p>Below are some selected publications.</p>  and recent blog posts-->

						<ul class="actions">
							<li><a href="#two" class="button">Publications</a></li>
							<!-- <li><a href="#three" class="button">Blog Posts</a></li>-->
							<!-- <li><a href="" class="button">Resume<i class="brands fa fa-download" style="margin-left:0.3em;" aria-hidden="true"></i></a></li>-->
							<li><a href="#three" class="button">Selected Talks</a></li>
							<li><a href="#four" class="button">Contact</a></li>
						</ul>
					</section>
					








					<section id="news">
		<h2>News</h2>
		<p>&bull;	I am excited to announce that I will join the <a href="https://dm3l.uzh.ch/home">University of Zurich</a> as an assistant professor in August this year! <br><br>
			<b>I am looking for </b><span style="color:#e62673">PhD students and postdocs</span> <b>starting from the fall.</b> If you are interested in working with me on topics related to optimization, federated learning, machine learning, privacy, and unlearning please apply!<br>
<span style="color:#787878">To apply, please email me (anakolos[at]stanford[dot]edu) and include the following documents combined in one PDF file:</span><br>
- short motivation letter (aim for max 150 words). Please explain your interests and why you are a good fit.<br>
- CV (please include any relevant experience and list of publications if applicable)<br>
- Transcripts of bachelor’s and master’s degree<br>
- Contact info of recommender letter writers (do not attach recommendations)<br>
- One of the previous research papers or previous thesis if applicable
		</p>

		<p>&bull;	12/2024: I was honored to receive <a href="https://ellis.eu/news/the-ellis-phd-award-2024-recognizes-three-young-scientists-from-universities-in-lausanne-london-tubingen">ELLIS PhD Awards</a> and <a href="https://www.epfl.ch/education/phd/phd-awards/thesis-distinction/">EPFL Thesis Distinction Award</a> for my PhD thesis! </p>

		<p>&bull;	10/2024: I am honored to be selected as a <a href="https://risingstars-eecs.mit.edu/">Rising Star in EECS</a> by MIT! </p>

		<p> &bull;	02/2024: I successfully defended my PhD thesis named "Optimization Algorithms for Decentralized, Distributed and Collaborative Machine Learning". <a href="https://infoscience.epfl.ch/record/307537?v=pdf">Link to thesis</a> </p>
					</section>







					<section id="two">
		<h2>Publications</h2>

						<ul class="papers">
								<li class="paper">
								<strong>On Convergence of Incremental Gradient for Non-Convex Smooth Functions</strong><br>
								<span><u>Anastasia Koloskova</u>, Nikita Doikov, Sebastian U. Stich, Martin Jaggi</span><br>
								<span style="font-size:smaller;"> ICML 2024 •
									<a href="https://arxiv.org/pdf/2305.19259">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@inproceedings{koloskova2024convergence,
      title={On Convergence of Incremental Gradient for Non-Convex Smooth Functions}, 
      author={Anastasia Koloskova and Nikita Doikov and Sebastian U. Stich and Martin Jaggi},
      year={2024},
      booktitle={Proceedings of the 41st International Conference on Machine Learning},
}</code>
								</span>
							</li>
							<li class="paper">
								<strong>The Privacy Power of Correlated Noise in Decentralized Learning</strong><br>
								<span>Youssef Allouah, <u>Anastasia Koloskova</u>, Aymane El Firdoussi, Martin Jaggi, Rachid Guerraoui</span><br>
								<span style="font-size:smaller;"> ICML 2024 •
									<a href="https://arxiv.org/pdf/2405.01031">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@inproceedings{allouah2024privacy,
      title={The Privacy Power of Correlated Noise in Decentralized Learning}, 
      author={Youssef Allouah and Anastasia Koloskova and Aymane El Firdoussi and Martin Jaggi and Rachid Guerraoui},
      year={2024},
      booktitle={Proceedings of the 41st International Conference on Machine Learning},
}</code>
								</span>
							</li>

							<li class="paper">
								<strong>Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization</strong><br>
								<span>Mathieu Even, <u>Anastasia Koloskova</u>, Laurent Massoulié</span><br>
								<span style="font-size:smaller;"> AISTATS 2024 •
									<a href="https://proceedings.mlr.press/v238/even24a/even24a.pdf">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@InProceedings{pmlr-v238-even24a,
  title = 	 { Asynchronous {SGD} on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization },
  author =       {Even, Mathieu and Koloskova, Anastasia and Massoulie, Laurent},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {64--72},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/even24a/even24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/even24a.html},
  abstract = 	 { Decentralized and asynchronous communications are two popular techniques to speedup communication complexity of distributed machine learning, by respectively removing the dependency over a central orchestrator and the need for synchronization. Yet, combining these two techniques together still remains a challenge. In this paper, we take a step in this direction and introduce Asynchronous SGD on Graphs (AGRAF SGD) — a general algorithmic framework that covers asynchronous versions of many popular algorithms including SGD, Decentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and computation assumptions. We provide rates of convergence under much milder assumptions than previous decentralized asynchronous works, while still recovering or even improving over the best know results for all the algorithms covered. }
}
									</code>
								</span>
							</li>

							<li class="paper">
								<strong>Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy</strong><br>
								<span><u>Anastasia Koloskova</u>, Ryan McKenna, Zachary Charles, Keith Rush, Brendan McMahan</span><br>
								<span style="font-size:smaller;"> NeurIPS 2023 •
									<a href="https://openreview.net/pdf?id=qCglMj6A4z">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@inproceedings{
	koloskova2023gradient,
	title={Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy},
	author={Anastasia Koloskova and Ryan McKenna and Zachary Charles and J Keith Rush and Hugh Brendan McMahan},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023},
	url={https://openreview.net/forum?id=qCglMj6A4z}
	}
									</code>
								</span>
							</li>

							<li class="paper">
								<strong>Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees</strong><br>
								<span><u>Anastasia Koloskova</u><span class="equalfirst">*</span>, Hadrien Hendrikx<span class="equalfirst">*</span>, Sebastian U Stich</span><br>
								<span style="font-size:smaller;"> ICML 2023 •
									<a href="https://openreview.net/pdf?id=C3DXiFTrve">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@inproceedings{koloskova:clipping,
	TITLE = {{Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees}},
	AUTHOR = {Koloskova, Anastasia and Hendrikx, Hadrien and Stich, Sebastian U.},
	URL = {https://openreview.net/pdf?id=C3DXiFTrve},
	BOOKTITLE = {{ICML 2023 - 40th International Conference on Machine Learning}},
	ADDRESS = {Honolulu, Hawaii,, United States},
	PAGES = {1-19},
	YEAR = {2023},
	MONTH = Jul,
	}									</code>
								</span>
							</li>

						<li class="paper">
							<strong>Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning</strong><br>
							<span><u>Anastasia Koloskova</u>, Sebastian U Stich, Martin Jaggi</span><br>
							<span style="font-size:smaller;">NeurIPS 2022 <span style="color:#e62673">(Oral, Notable top 7%)</span> •
								<a target="_blank" rel="noopener noreferrer" href="https://openreview.net/forum?id=4_oCZgBIVI">Paper</a> •
								<a href="" onclick="showBibTex($(this)); return false">BibTex</a>
								<code class="bibtex">
@inproceedings{
	koloskova2022sharper,
	title={Sharper Convergence Guarantees for Asynchronous {SGD} for Distributed and Federated Learning},
	author={Anastasia Koloskova and Sebastian U Stich and Martin Jaggi},
	booktitle={Advances in Neural Information Processing Systems},
	editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
	year={2022},
	url={https://openreview.net/forum?id=4_oCZgBIVI}
	}
								</code>
							</span>
						</li>

			<li class="paper">
				<strong>Decentralized Local Stochastic Extra-Gradient for Variational Inequalities</strong><br>
				<span>Aleksandr Beznosikov, Pavel Dvurechensky, <u>Anastasia Koloskova</u>, Valentin Samokhin, Sebastian U. Stich, Alexander Gasnikov</span><br>
				<span style="font-size:smaller;">NeurIPS 2022 •
					<a target="_blank" rel="noopener noreferrer" href="https://openreview.net/pdf?id=Y4vT7m4e3d">Paper</a> •
					<a href="" onclick="showBibTex($(this)); return false">BibTex</a>
					<code class="bibtex">
@inproceedings{
	beznosikov2022decentralized,
	title={Decentralized Local Stochastic Extra-Gradient for Variational Inequalities},
	author={Aleksandr Beznosikov and Pavel Dvurechensky and Anastasia Koloskova and Valentin Samokhin and Sebastian U Stich and Alexander Gasnikov},
	booktitle={Advances in Neural Information Processing Systems},
	editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
	year={2022},
	url={https://openreview.net/forum?id=Y4vT7m4e3d}
	}
					</code>
				</span>
				<a href="#"></a>
			</li>


			<li class="paper">
				<strong>An Improved Analysis of Gradient Tracking for Decentralized Machine Learning</strong><br>
				<span><u>Anastasia Koloskova</u>, Tao Lin, Sebastian U Stich</span><br>
				<span style="font-size:smaller;">NeurIPS 2021 •
					<a href="https://openreview.net/forum?id=CmI7NqBR4Ua">Paper</a> • 
					<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
				  <code class="bibtex">
@inproceedings{
	koloskova2021an,
	title={An Improved Analysis of Gradient Tracking for Decentralized Machine Learning},
	author={Anastasia Koloskova and Tao Lin and Sebastian U Stich},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021},
	url={https://openreview.net/forum?id=CmI7NqBR4Ua}
}
					</code>
			  </span>
			</li>

						<li class="paper">
							<strong>RelaySum for Decentralized Deep Learning on Heterogeneous Data</strong><br>
							<span>Thijs Vogels<span class="equalfirst">*</span>, Lie He<span class="equalfirst">*</span>, <u>Anastasia Koloskova</u>, Sai Praneeth Karimireddy, Tao Lin, Sebastian U Stich, Martin Jaggi</span><br>
							<span style="font-size:smaller;">NeurIPS 2021 •
								<a target="_blank" rel="noopener noreferrer" href="https://openreview.net/forum?id=Qo6kYy4SBI-">Paper</a> •
								<a target="_blank" rel="noopener noreferrer" href="https://github.com/epfml/relaysgd">Code</a> •
								<a href="" onclick="showBibTex($(this)); return false">BibTex</a>
								<code class="bibtex">
@inproceedings{
	vogels2021relaysum,
	title={RelaySum for Decentralized Deep Learning on Heterogeneous Data},
	author={Thijs Vogels and Lie He and Anastasia Koloskova and Sai Praneeth Karimireddy and Tao Lin and Sebastian U Stich and Martin Jaggi},
	booktitle={Advances in Neural Information Processing Systems},
	editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
	year={2021},
	url={https://openreview.net/forum?id=Qo6kYy4SBI-}
	}										
								</code>
							</span>
							<a href="#"></a>
						</li>



	<li class="paper">
		<strong>A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!</strong><br>
		<span>Dmitry Kovalev, <u>Anastasia Koloskova</u>, Martin Jaggi, Peter Richtárik, Sebastian U. Stich</span><br>
		<span style="font-size:smaller;">AISTATS 2021 •
			<a href="http://proceedings.mlr.press/v130/kovalev21a/kovalev21a.pdf">Paper</a> • 
			<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
			<code class="bibtex">
@InProceedings{pmlr-v130-kovalev21a,
	title = 	 { A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free! },
	author =       {Kovalev, Dmitry and Koloskova, Anastasia and Jaggi, Martin and Richtarik, Peter and Stich, Sebastian},
	booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {4087--4095},
	year = 	 {2021},
	editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
	volume = 	 {130},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--15 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v130/kovalev21a/kovalev21a.pdf},
	url = 	 {https://proceedings.mlr.press/v130/kovalev21a.html},
	abstract = 	 { Decentralized optimization methods enable on-device training of machine learning models without a central coordinator. In many scenarios communication between devices is energy demanding and time consuming and forms the bottleneck of the entire system. We propose a new randomized first-order method which tackles the communication bottleneck by applying randomized compression operators to the communicated messages. By combining our scheme with a new variance reduction technique that progressively throughout the iterations reduces the adverse effect of the injected quantization noise, we obtain a scheme that converges linearly on strongly convex decentralized problems while using compressed communication only. We prove that our method can solve the problems without any increase in the number of communications compared to the baseline which does not perform any communication compression while still allowing for a significant compression factor which depends on the conditioning of the problem and the topology of the network. We confirm our theoretical findings in numerical experiments. }
	}
			</code>
		</span>
	</li>

	<li class="paper">
		<strong>Consensus Control for Decentralized Deep Learning</strong><br>
		<span>Lingjing Kong, Tao Lin, <u>Anastasia Koloskova</u>, Martin Jaggi, Sebastian U. Stich</span><br>
		<span style="font-size:smaller;">ICML 2021 •
			<a href="http://proceedings.mlr.press/v139/kong21a/kong21a.pdf">Paper</a> • 
			<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
			<code class="bibtex">
@InProceedings{pmlr-v139-kong21a,
	title = 	 {Consensus Control for Decentralized Deep Learning},
	author =       {Kong, Lingjing and Lin, Tao and Koloskova, Anastasia and Jaggi, Martin and Stich, Sebastian},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {5686--5696},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {18--24 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/kong21a/kong21a.pdf},
	url = 	 {https://proceedings.mlr.press/v139/kong21a.html},
	abstract = 	 {Decentralized training of deep learning models enables on-device learning over networks, as well as efficient scaling to large compute clusters. Experiments in earlier works reveal that, even in a data-center setup, decentralized training often suffers from the degradation in the quality of the model: the training and test performance of models trained in a decentralized fashion is in general worse than that of models trained in a centralized fashion, and this performance drop is impacted by parameters such as network size, communication topology and data partitioning. We identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. We show in theory that when the training consensus distance is lower than a critical quantity, decentralized training converges as fast as the centralized counterpart. We empirically validate that the relation between generalization performance and consensus distance is consistent with this theoretical observation. Our empirical insights allow the principled design of better decentralized training schemes that mitigate the performance drop. To this end, we provide practical training guidelines and exemplify its effectiveness on the data-center setup as the important first step.}
	}				  
			</code>
		</span>
	</li>

	<li class="paper">
		<strong>A Unified Theory of Decentralized SGD with Changing Topology and Local Updates</strong><br>
		<span><u>Anastasia Koloskova</u><span class="equalfirst">*</span>, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, Sebastian U. Stich<span class="equalfirst">*</span></span><br>
		<span style="font-size:smaller;">ICML 2020 •
			<a href="https://proceedings.mlr.press/v119/koloskova20a/koloskova20a.pdf">Paper</a> • 
			<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
			<code class="bibtex">
@InProceedings{pmlr-v119-koloskova20a,
	title = 	 {A Unified Theory of Decentralized {SGD} with Changing Topology and Local Updates},
	author =       {Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
	booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
	pages = 	 {5381--5393},
	year = 	 {2020},
	editor = 	 {III, Hal Daumé and Singh, Aarti},
	volume = 	 {119},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--18 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v119/koloskova20a/koloskova20a.pdf},
	url = 	 {https://proceedings.mlr.press/v119/koloskova20a.html},
	abstract = 	 {Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efficiency. In this paper we introduce a unified convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities. Our algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD).}
	}
</code>
		</span>
	</li>

	<li class="paper">
		<strong>Decentralized Deep Learning with Arbitrary Communication Compression</strong><br>
		<span><u>Anastasia Koloskova</u><span class="equalfirst">*</span>, Tao Lin<span class="equalfirst">*</span>, Sebastian U. Stich, Martin Jaggi</span><br>
		<span style="font-size:smaller;">ICLR 2020 •
			<a href="https://openreview.net/forum?id=SkgGCkrKvH">Paper</a> • 
			<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
			<code class="bibtex">
@inproceedings{
	Koloskova*2020Decentralized,
	title={Decentralized Deep Learning with Arbitrary Communication Compression},
	author={Anastasia Koloskova* and Tao Lin* and Sebastian U Stich and Martin Jaggi},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=SkgGCkrKvH}
	}
</code>
		</span>
	</li>

	<li class="paper">
		<strong>Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication</strong><br>
		<span><u>Anastasia Koloskova</u><span class="equalfirst">*</span>, Sebastian U. Stich<span class="equalfirst">*</span>, Martin Jaggi</span><br>
		<span style="font-size:smaller;">ICML 2019 •
			<a href="https://proceedings.mlr.press/v97/koloskova19a/koloskova19a.pdf">Paper</a> • 
			<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
			<code class="bibtex">
@InProceedings{pmlr-v97-koloskova19a,
	title = 	 {Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication},
	author =       {Koloskova, Anastasia and Stich, Sebastian and Jaggi, Martin},
	booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
	pages = 	 {3478--3487},
	year = 	 {2019},
	editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume = 	 {97},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {09--15 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v97/koloskova19a/koloskova19a.pdf},
	url = 	 {https://proceedings.mlr.press/v97/koloskova19a.html},
	abstract = 	 {We consider decentralized stochastic optimization with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a fixed communication graph. To address the communication bottleneck, the nodes compress (e.g. quantize or sparsify) their model updates. We cover both unbiased and biased compression operators with quality denoted by \delta &lt;= 1 (\delta=1 meaning no compression). We (i) propose a novel gossip-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1/(nT) + 1/(T \rho^2 \delta)^2) for strongly convex objectives, where T denotes the number of iterations and \rho the eigengap of the connectivity matrix. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the average consensus problem that converges in time O(1/(\rho^2\delta) \log (1/\epsilon)) for accuracy \epsilon &gt; 0. This is (up to our knowledge) the first gossip algorithm that supports arbitrary compressed messages for \delta &gt; 0 and still exhibits linear convergence. We (iii) show in experiments that both of our algorithms do outperform the respective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes.}
	}
	</code>
		</span>
	</li>

	<li class="paper">
		<strong>Efficient Greedy Coordinate Descent for Composite Problems</strong><br>
		<span>Sai Praneeth Karimireddy<span class="equalfirst">*</span>, <u>Anastasia Koloskova</u><span class="equalfirst">*</span>, Sebastian U. Stich, Martin Jaggi</span><br>
		<span style="font-size:smaller;">AISTATS 2019 •
			<a href="https://arxiv.org/abs/1810.06999">Paper</a> • 
			<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
			<code class="bibtex">
@inproceedings{Karimireddy:271570,
	title = {Efficient Greedy Coordinate Descent for Composite  Problems},
	author = {Karimireddy, Sai Praneeth Reddy and Koloskova, Anastasiia  and Stich, Sebastian Urban and Jaggi, Martin},
	journal = {The 22nd International Conference on Artificial  Intelligence and Statistics, 16-18 April 2019},
	volume = {89},
	series = {Proceedings of Machine Learning Research. 89},
	pages = {2887-2896},
	year = {2019},
	abstract = {Coordinate descent with random coordinate selection is the  current state of the art for many large scale optimization  problems. However, greedy selection of the steepest  coordinate on smooth problems can yield convergence rates  independent of the dimension n, requiring n times fewer  iterations. In this paper, we consider greedy updates that  are based on subgradients for a class of non-smooth  composite problems, including L1-regularized problems, SVMs  and related applications. For these problems we provide (i)  the first linear rates of convergence independent of n, and  show that our greedy update rule provides speedups similar  to those obtained in the smooth case. This was previously  conjectured to be true for a stronger greedy coordinate  selection strategy. Furthermore, we show that (ii) our new  selection rule can be mapped to instances of maximum inner  product search, allowing to leverage standard nearest  neighbor algorithms to speed up the implementation. We  demonstrate the validity of the approach through extensive  numerical experiments.},
	url = {http://infoscience.epfl.ch/record/271570},
}	</code>
		</span>
	</li>

					</ul>
			

				<!-- 		<h2>Workshop Papers</h2>

						<ul class="papers">

							<li class="paper">
								<strong>Fast Causal Attention with Dynamic Sparsity</strong><br>
								<span><u>M. Pagliardini</u><span class="equalfirst">*</span>, D. Paliotta<span class="equalfirst">*</span>, M. Jaggi, F. Fleuret</span><br>
								<span style="font-size:smaller;">Workshop on Efficient Systems for Foundation Models (ICML 2023) <span style="color:#e62673">(Oral)</span> •
									<a href="https://arxiv.org/abs/2306.01160">Paper</a> • <a href="https://github.com/epfml/dynamic-sparse-flash-attention">Code</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@misc{pagliardini2023faster,
	title={Faster Causal Attention Over Large Sequences Through Sparse Flash Attention}, 
	author={Matteo Pagliardini and Daniele Paliotta and Martin Jaggi and François Fleuret},
	year={2023},
	eprint={2306.01160},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
									</code>
								</span>
							</li>

							<li class="paper">
								<strong>Diversity through Disagreement for Better Transferability</strong><br>
								<span><u>Matteo Pagliardini</u>, Martin Jaggi, François Fleuret, Sai Praneeth Karimireddy</span><br>
								<span style="font-size:smaller;">DistShift Workshop on Distribution Shifts (NeurIPS 2022) •
									<a href="https://openreview.net/pdf?id=gwvb94JWzI">Paper</a> • <a href="https://github.com/mpagli/Agree-to-Disagree">Code</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@inproceedings{PagliardiniJFK23,
	author       = {Matteo Pagliardini and
					Martin Jaggi and
					Fran{\c{c}}ois Fleuret and
					Sai Praneeth Karimireddy},
	title        = {Agree to Disagree: Diversity through Disagreement for Better Transferability},
	booktitle    = {{ICLR}},
	publisher    = {OpenReview.net},
	year         = {2023}
}
									</code>
								</span>
							</li>


							<li class="paper">
								<strong>The Peril of Popular Deep Learning Uncertainty Estimation Methods</strong><br>
								<span>Yehao Liu<span class="equalfirst">*</span>, <u>Matteo Pagliardini</u><span class="equalfirst">*</span>, Tatjana Chavdarova, Sebastian U. Stich</span><br>
								<span style="font-size:smaller;">Workshop on Bayesian Deep Learning (NeurIPS 2021) •
									<a target="_blank" rel="noopener noreferrer"  href="">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@article{liu2021peril,
 title   = {The Peril of Popular Deep Learning Uncertainty Estimation Methods},
 author  = {Yehao Liu and Matteo Pagliardini and Tatjana Chavdarova and Sebastian U. Stich},
 booktitle= {NeurIPS Workshop on Bayesian Deep Learning},
 year    = {2021},
}
									</code>
								</span>
								<a href="#"></a>
							</li>

							<li class="paper">
								<strong>Improved Adversarial Robustness via Uncertainty Targeted Attacks</strong><br>
								<span>Gilberto Manunza<span class="equalfirst">*</span>, <u>Matteo Pagliardini</u><span class="equalfirst">*</span>, Martin Jaggi, Tatjana Chavdarova</span><br>
								<span style="font-size:smaller;">Workshop on Uncertainty and Robustness in Deep Learning (ICML 2021) •
									<a target="_blank" rel="noopener noreferrer"  href="https://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-096.pdf">Paper</a> •
									<a href="#" onclick="showBibTex($(this)); return false">BibTex</a>
									<code class="bibtex">
@inproceedings{ManunzaPagliardini2021,
 title   = {Improved Adversarial Robustness via Uncertainty Targeted Attacks},
 author  = {Gilberto Manunza and Matteo Pagliardini and Martin Jaggi and Tatjana Chavdarova},
 booktitle= {ICML Workshop on Uncertainty and Robustness in Deep Learning},
 year    = {2021},
 url     = {http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-096.pdf}
}
									</code>
								</span>
								<a href="#"></a>
							</li>

						</ul>

						<p style="font-size:smaller;"><span class="equalfirst">*</span> Indicates equal contribution.</p>

						<div class="row">

						</div>

						<a href="https://scholar.google.ch/citations?user=FXacC3oAAAAJ&hl=en" class="button" target="_blank" rel="noopener noreferrer">Google Scholar</a>
 -->
					</section>

				<!-- Two -->
				  <!--
					<section id="three">
						<h2>Blog Posts</h2>
						<div class="row">
							<article class="col-6 col-12-xsmall work-item">
								<a href="posts/post1/index.html" class="image fit thumb"><img src="images/thumbs/01.jpg" alt="" /></a>
								<h3>A visual introduction to GANs</h3>
								<p>Build a toy GAN.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/02.jpg" class="image fit thumb"><img src="images/thumbs/02.jpg" alt="" /></a>
								<h3>The facinating bahavior of smooth games</h3>
								<p>An introduction to the weird world of smooth game optimization.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/03.jpg" class="image fit thumb"><img src="images/thumbs/03.jpg" alt="" /></a>
								<h3>Visualizing adversarial perturbations</h3>
								<p>How to visualize small perturbations crossing the boundary decision.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/04.jpg" class="image fit thumb"><img src="images/thumbs/04.jpg" alt="" /></a>
								<h3>Mutual information for source bias removal</h3>
								<p>TODO.</p>
							</article>
						</div>
					</section>
				  -->

					<section id="three">
						<h2>Selected Talks and Presentations</h2>
						<ul class="talks">
							<li class="talk"><strong>Methodological Aspects of Federated Learning, Challenges and Opportunities</strong><br>
								Basel Biometric Society, 2023 • <a target="_blank" rel="noopener noreferrer" href="https://baselbiometrics.github.io/home/docs/events_past.html#federated-learning-and-new-data-modalities">link</a>
							</li>
							<li class="talk"><strong>Convergence of Gradient Descent with Linearly Correlated Noise and Applications to Differentially Private Learning</strong><br>
								Simons Berkeley Federated & Collaborative Learning Workshop, 2023 • <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/live/kzU8DzgBBkE?feature=shared">link</a>
							</li>
							<li class="talk"><strong>Sharper Convergence Guarantees for Asynchronous SGD</strong><br>
								Federated Learning One World Seminar, 2022 • <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/vP6qzSGOjoo?feature=shared">link</a>
							</li>
							<li class="talk"><strong>A Unified Theory of Decentralized SGD with Changing Topology and Local Updates</strong><br>
								Federated Learning One World Seminar, 2021 • <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/K2tBNspkkYA?feature=shared">link</a>
							</li>
							<li class="talk"><strong>Choco-SGD: Communication Efficient Decentralized Learning</strong><br>
								Applied Machine Learning Days, 2020 • <a target="_blank" rel="noopener noreferrer" href="https://appliedmldays.org/highlights/121">link</a>
							</li>
							<li class="talk"><strong>Communication Efficient Decentralized Machine Learning</strong><br>
								Youtube video @ ZettaBytes, EPFL, 2019 • <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/93zl47WqLsw?feature=shared">link</a>
							</li>
							<li class="talk"><strong>Communication Efficient Decentralized Machine Learning</strong><br>
								IC (CS) Department Research Day, EPFL, 2019 • <a target="_blank" rel="noopener noreferrer" href="https://portal.klewel.com/watch/webcast/epfl-ic-research-day-2019/talk/10/">link</a>
							</li>
						</ul>
					</section>

				<!-- Three -->
					<section id="four">
						<h2>Get In Touch</h2>
						<strong>My email:</strong><p>anakolos@[stanford].edu</p>

					</section>

				<!-- Four -->

				<!--

					<section id="four">
						<h2>Elements</h2>

						<section>
							<h4>Text</h4>
							<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
							This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
							This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
							<hr />
							<header>
								<h4>Heading with a Subtitle</h4>
								<p>Lorem ipsum dolor sit amet nullam id egestas urna aliquam</p>
							</header>
							<p>Nunc lacinia ante nunc ac lobortis. Interdum adipiscing gravida odio porttitor sem non mi integer non faucibus ornare mi ut ante amet placerat aliquet. Volutpat eu sed ante lacinia sapien lorem accumsan varius montes viverra nibh in adipiscing blandit tempus accumsan.</p>
							<header>
								<h5>Heading with a Subtitle</h5>
								<p>Lorem ipsum dolor sit amet nullam id egestas urna aliquam</p>
							</header>
							<p>Nunc lacinia ante nunc ac lobortis. Interdum adipiscing gravida odio porttitor sem non mi integer non faucibus ornare mi ut ante amet placerat aliquet. Volutpat eu sed ante lacinia sapien lorem accumsan varius montes viverra nibh in adipiscing blandit tempus accumsan.</p>
							<hr />
							<h2>Heading Level 2</h2>
							<h3>Heading Level 3</h3>
							<h4>Heading Level 4</h4>
							<h5>Heading Level 5</h5>
							<h6>Heading Level 6</h6>
							<hr />
							<h5>Blockquote</h5>
							<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
							<h5>Preformatted</h5>
							<pre><code>i = 0;

while (!deck.isInOrder()) {
print 'Iteration ' + i;
deck.shuffle();
i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
						</section>

						<section>
							<h4>Lists</h4>
							<div class="row">
								<div class="col-6 col-12-xsmall">
									<h5>Unordered</h5>
									<ul>
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Sagittis adipiscing lorem eleifend.</li>
										<li>Felis enim feugiat dolore viverra.</li>
									</ul>
									<h5>Alternate</h5>
									<ul class="alt">
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Sagittis adipiscing lorem eleifend.</li>
										<li>Felis enim feugiat dolore viverra.</li>
									</ul>
								</div>
								<div class="col-6 col-12-xsmall">
									<h5>Ordered</h5>
									<ol>
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Etiam vel felis at lorem sed viverra.</li>
										<li>Felis enim feugiat dolore viverra.</li>
										<li>Dolor pulvinar etiam magna etiam.</li>
										<li>Etiam vel felis at lorem sed viverra.</li>
										<li>Felis enim feugiat dolore viverra.</li>
									</ol>
									<h5>Icons</h5>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
										<li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li>
										<li><a href="#" class="icon brands fa-tumblr"><span class="label">Tumblr</span></a></li>
									</ul>
								</div>
							</div>
							<h5>Actions</h5>
							<ul class="actions">
								<li><a href="#" class="button primary">Default</a></li>
								<li><a href="#" class="button">Default</a></li>
							</ul>
							<ul class="actions small">
								<li><a href="#" class="button primary small">Small</a></li>
								<li><a href="#" class="button small">Small</a></li>
							</ul>
							<div class="row">
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</div>
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary small">Small</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
								</div>
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary fit">Default</a></li>
										<li><a href="#" class="button fit">Default</a></li>
									</ul>
								</div>
								<div class="col-6 col-12-small">
									<ul class="actions stacked">
										<li><a href="#" class="button primary small fit">Small</a></li>
										<li><a href="#" class="button small fit">Small</a></li>
									</ul>
								</div>
							</div>
						</section>

						<section>
							<h4>Table</h4>
							<h5>Default</h5>
							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th>Name</th>
											<th>Description</th>
											<th>Price</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>Item One</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Two</td>
											<td>Vis ac commodo adipiscing arcu aliquet.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Three</td>
											<td> Morbi faucibus arcu accumsan lorem.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Four</td>
											<td>Vitae integer tempus condimentum.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Five</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
									</tbody>
									<tfoot>
										<tr>
											<td colspan="2"></td>
											<td>100.00</td>
										</tr>
									</tfoot>
								</table>
							</div>

							<h5>Alternate</h5>
							<div class="table-wrapper">
								<table class="alt">
									<thead>
										<tr>
											<th>Name</th>
											<th>Description</th>
											<th>Price</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td>Item One</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Two</td>
											<td>Vis ac commodo adipiscing arcu aliquet.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Three</td>
											<td> Morbi faucibus arcu accumsan lorem.</td>
											<td>29.99</td>
										</tr>
										<tr>
											<td>Item Four</td>
											<td>Vitae integer tempus condimentum.</td>
											<td>19.99</td>
										</tr>
										<tr>
											<td>Item Five</td>
											<td>Ante turpis integer aliquet porttitor.</td>
											<td>29.99</td>
										</tr>
									</tbody>
									<tfoot>
										<tr>
											<td colspan="2"></td>
											<td>100.00</td>
										</tr>
									</tfoot>
								</table>
							</div>
						</section>

						<section>
							<h4>Buttons</h4>
							<ul class="actions">
								<li><a href="#" class="button primary">Primary</a></li>
								<li><a href="#" class="button">Default</a></li>
							</ul>
							<ul class="actions">
								<li><a href="#" class="button large">Large</a></li>
								<li><a href="#" class="button">Default</a></li>
								<li><a href="#" class="button small">Small</a></li>
							</ul>
							<ul class="actions fit">
								<li><a href="#" class="button primary fit">Fit</a></li>
								<li><a href="#" class="button fit">Fit</a></li>
							</ul>
							<ul class="actions fit small">
								<li><a href="#" class="button primary fit small">Fit + Small</a></li>
								<li><a href="#" class="button fit small">Fit + Small</a></li>
							</ul>
							<ul class="actions">
								<li><a href="#" class="button primary icon solid fa-download">Icon</a></li>
								<li><a href="#" class="button icon solid fa-download">Icon</a></li>
							</ul>
							<ul class="actions">
								<li><span class="button primary disabled">Primary</span></li>
								<li><span class="button disabled">Default</span></li>
							</ul>
						</section>

						<section>
							<h4>Form</h4>
							<form method="post" action="#">
								<div class="row gtr-uniform gtr-50">
									<div class="col-6 col-12-xsmall">
										<input type="text" name="demo-name" id="demo-name" value="" placeholder="Name" />
									</div>
									<div class="col-6 col-12-xsmall">
										<input type="email" name="demo-email" id="demo-email" value="" placeholder="Email" />
									</div>
									<div class="col-12">
										<select name="demo-category" id="demo-category">
											<option value="">- Category -</option>
											<option value="1">Manufacturing</option>
											<option value="1">Shipping</option>
											<option value="1">Administration</option>
											<option value="1">Human Resources</option>
										</select>
									</div>
									<div class="col-4 col-12-small">
										<input type="radio" id="demo-priority-low" name="demo-priority" checked>
										<label for="demo-priority-low">Low Priority</label>
									</div>
									<div class="col-4 col-12-small">
										<input type="radio" id="demo-priority-normal" name="demo-priority">
										<label for="demo-priority-normal">Normal Priority</label>
									</div>
									<div class="col-4 col-12-small">
										<input type="radio" id="demo-priority-high" name="demo-priority">
										<label for="demo-priority-high">High Priority</label>
									</div>
									<div class="col-6 col-12-small">
										<input type="checkbox" id="demo-copy" name="demo-copy">
										<label for="demo-copy">Email me a copy of this message</label>
									</div>
									<div class="col-6 col-12-small">
										<input type="checkbox" id="demo-human" name="demo-human" checked>
										<label for="demo-human">I am a human and not a robot</label>
									</div>
									<div class="col-12">
										<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
									</div>
									<div class="col-12">
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="primary" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</div>
								</div>
							</form>
						</section>

						<section>
							<h4>Image</h4>
							<h5>Fit</h5>
							<div class="box alt">
								<div class="row gtr-50 gtr-uniform">
									<div class="col-12"><span class="image fit"><img src="images/fulls/05.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/01.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/02.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/03.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/04.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/05.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/06.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/03.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/02.jpg" alt="" /></span></div>
									<div class="col-4"><span class="image fit"><img src="images/thumbs/01.jpg" alt="" /></span></div>
								</div>
							</div>
							<h5>Left &amp; Right</h5>
							<p><span class="image left"><img src="images/avatar.jpg" alt="" /></span>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent.</p>
							<p><span class="image right"><img src="images/avatar.jpg" alt="" /></span>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent tincidunt felis sagittis eget. tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan eu faucibus. Integer ac pellentesque praesent.</p>
						</section>

					</section>
				-->


			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<li><a href="https://twitter.com/Ana_koloskova" class="icon brands fa-twitter" target="_blank" rel="noopener noreferrer"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/koloskova" class="icon brands fa-github" target="_blank" rel="noopener noreferrer"><span class="label">Github</span></a></li>
						<li><a href="https://scholar.google.com/citations?user=ldJpvE8AAAAJ&hl=en" target="_blank" rel="noopener noreferrer" class="brands fa fa-graduation-cap" style="text-decoration:none;border-bottom:none;position:relative;font-size:1.5em;"><span style="display:none" class="label">Scholar</span></a></li>
						<li><a href="#four" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; 2023</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
